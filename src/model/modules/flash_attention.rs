//! Flash Attention Implementation
//!
//! Memory-efficient attention that computes attention scores in tiles,
//! achieving O(N) memory complexity instead of O(N^2).
//!
//! # Algorithm
//! Flash Attention uses:
//! - Tiled computation to keep memory bounded
//! - Online softmax normalization (running max and sum)
//! - Single pass over K/V cache per query
//!
//! # Performance
//! - RAM reduction: ~50% for long sequences (vs standard attention)
//! - Speed: Comparable or faster due to better memory locality
//!
//! # References
//! - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
//!   (Dao et al., 2022)
#![allow(clippy::needless_range_loop)]

use ndarray::{Array2, ArrayView2};

/// Default tile size for tiled attention computation.
/// This is tuned for typical CPU cache sizes (L2 cache).
pub const DEFAULT_TILE_SIZE: usize = 64;

/// Minimum sequence length to use flash attention.
/// For short sequences, standard attention is more efficient.
pub const FLASH_ATTENTION_THRESHOLD: usize = 128;

/// Configuration for flash attention.
#[derive(Debug, Clone, Copy)]
pub struct FlashAttentionConfig {
    /// Tile size for tiled computation (default: 64)
    pub tile_size: usize,
    /// Minimum sequence length to enable flash attention
    pub threshold: usize,
    /// Whether to use parallel computation for tiles
    pub parallel: bool,
}

impl Default for FlashAttentionConfig {
    fn default() -> Self {
        Self {
            tile_size: DEFAULT_TILE_SIZE,
            threshold: FLASH_ATTENTION_THRESHOLD,
            parallel: cfg!(feature = "parallel"),
        }
    }
}

impl FlashAttentionConfig {
    /// Create configuration optimized for memory efficiency
    pub fn memory_optimized() -> Self {
        Self {
            tile_size: 32, // Smaller tiles use less memory
            threshold: 64,
            parallel: false, // Serial uses less peak memory
        }
    }

    /// Create configuration optimized for speed
    pub fn speed_optimized() -> Self {
        Self {
            tile_size: 128, // Larger tiles for better throughput
            threshold: 256, // Only use for longer sequences
            parallel: true,
        }
    }
}

/// Online softmax state for incremental computation.
/// Tracks running max and sum to compute exact softmax without materializing full attention matrix.
#[derive(Debug, Clone)]
struct OnlineSoftmaxState {
    /// Running maximum value seen so far
    max: f32,
    /// Running sum of exp(x - max) values
    sum: f32,
    /// Accumulated output (weighted sum of values)
    output: Vec<f32>,
}

impl OnlineSoftmaxState {
    fn new(head_dim: usize) -> Self {
        Self {
            max: f32::NEG_INFINITY,
            sum: 0.0,
            output: vec![0.0; head_dim],
        }
    }

    /// Update the online softmax state with a new tile of attention scores and values.
    ///
    /// This implements the online softmax algorithm:
    /// 1. Compute new max considering the tile
    /// 2. Rescale previous sum and output by exp(old_max - new_max)
    /// 3. Add new contributions: exp(scores - new_max) * values
    /// 4. Update running sum
    #[inline]
    fn update_tile(&mut self, scores: &[f32], values: &[f32], head_dim: usize, tile_len: usize) {
        // Find max in this tile
        let tile_max = scores
            .iter()
            .take(tile_len)
            .copied()
            .fold(f32::NEG_INFINITY, f32::max);

        // Compute new global max
        let new_max = self.max.max(tile_max);

        // Rescale previous contributions if max changed
        if new_max > self.max && self.sum > 0.0 {
            let scale = (self.max - new_max).exp();
            self.sum *= scale;
            for o in self.output.iter_mut() {
                *o *= scale;
            }
        }

        // Compute exp(scores - new_max) and accumulate weighted values
        let mut tile_sum = 0.0f32;
        for i in 0..tile_len {
            let weight = (scores[i] - new_max).exp();
            tile_sum += weight;

            // Add weighted value to output
            let value_start = i * head_dim;
            for d in 0..head_dim {
                self.output[d] += weight * values[value_start + d];
            }
        }

        self.sum += tile_sum;
        self.max = new_max;
    }

    /// Finalize the softmax computation by dividing by the total sum.
    #[inline]
    fn finalize(&mut self) {
        if self.sum > 0.0 {
            let inv_sum = 1.0 / self.sum;
            for o in self.output.iter_mut() {
                *o *= inv_sum;
            }
        }
    }

    /// Get the final output after finalization.
    fn output(&self) -> &[f32] {
        &self.output
    }
}

/// Flash Attention for a single head.
///
/// Computes attention for one query head against the K/V cache using
/// tiled computation and online softmax.
///
/// # Arguments
/// * `query` - Query vector [head_dim]
/// * `k_cache` - Key cache [seq_len, head_dim]
/// * `v_cache` - Value cache [seq_len, head_dim]
/// * `seq_len` - Current sequence length (how much of cache is valid)
/// * `scale` - Attention scale factor (typically 1/sqrt(head_dim))
/// * `config` - Flash attention configuration
///
/// # Returns
/// Attention output [head_dim]
pub fn flash_attention_single_head(
    query: &[f32],
    k_cache: ArrayView2<f32>,
    v_cache: ArrayView2<f32>,
    seq_len: usize,
    scale: f32,
    config: &FlashAttentionConfig,
) -> Vec<f32> {
    let head_dim = query.len();
    let tile_size = config.tile_size.min(seq_len);

    // Use standard attention for short sequences
    if seq_len < config.threshold {
        return standard_attention_single_head(query, k_cache, v_cache, seq_len, scale);
    }

    let mut state = OnlineSoftmaxState::new(head_dim);

    // Pre-allocate tile buffers
    let mut tile_scores = vec![0.0f32; tile_size];
    let mut tile_values = vec![0.0f32; tile_size * head_dim];

    // Process K/V cache in tiles
    let mut pos = 0;
    while pos < seq_len {
        let tile_end = (pos + tile_size).min(seq_len);
        let current_tile_size = tile_end - pos;

        // Compute attention scores for this tile: score[i] = query Â· k[i] * scale
        for i in 0..current_tile_size {
            let k_row = k_cache.row(pos + i);
            let k_slice = k_row.as_slice().expect("k_cache row must be contiguous");
            let mut dot = 0.0f32;
            for d in 0..head_dim {
                dot += query[d] * k_slice[d];
            }
            tile_scores[i] = dot * scale;
        }

        // Copy values for this tile
        for i in 0..current_tile_size {
            let v_row = v_cache.row(pos + i);
            let v_slice = v_row.as_slice().expect("v_cache row must be contiguous");
            for d in 0..head_dim {
                tile_values[i * head_dim + d] = v_slice[d];
            }
        }

        // Update online softmax state
        state.update_tile(&tile_scores, &tile_values, head_dim, current_tile_size);

        pos = tile_end;
    }

    // Finalize and return output
    state.finalize();
    state.output().to_vec()
}

/// Standard attention for a single head (fallback for short sequences).
fn standard_attention_single_head(
    query: &[f32],
    k_cache: ArrayView2<f32>,
    v_cache: ArrayView2<f32>,
    seq_len: usize,
    scale: f32,
) -> Vec<f32> {
    let head_dim = query.len();

    // Compute all attention scores
    let mut scores = vec![0.0f32; seq_len];
    for i in 0..seq_len {
        let k_row = k_cache.row(i);
        let k_slice = k_row.as_slice().expect("k_cache row must be contiguous");
        let mut dot = 0.0f32;
        for d in 0..head_dim {
            dot += query[d] * k_slice[d];
        }
        scores[i] = dot * scale;
    }

    // Softmax
    let max_score = scores.iter().copied().fold(f32::NEG_INFINITY, f32::max);
    let mut sum = 0.0f32;
    for s in scores.iter_mut() {
        *s = (*s - max_score).exp();
        sum += *s;
    }
    let inv_sum = 1.0 / sum;
    for s in scores.iter_mut() {
        *s *= inv_sum;
    }

    // Weighted sum of values
    let mut output = vec![0.0f32; head_dim];
    for i in 0..seq_len {
        let v_row = v_cache.row(i);
        let v_slice = v_row.as_slice().expect("v_cache row must be contiguous");
        let w = scores[i];
        for d in 0..head_dim {
            output[d] += w * v_slice[d];
        }
    }

    output
}

/// Flash Attention for multiple heads (grouped-query attention support).
///
/// Computes attention for all query heads against the shared K/V cache.
/// Supports grouped-query attention (GQA) where multiple query heads share
/// the same K/V head.
///
/// # Arguments
/// * `q_state` - Query states [n_heads, head_dim]
/// * `k_cache` - Key cache for one layer [n_kv_heads, seq_len, head_dim]
/// * `v_cache` - Value cache for one layer [n_kv_heads, seq_len, head_dim]
/// * `seq_len` - Current sequence length
/// * `config` - Flash attention configuration
///
/// # Returns
/// Attention output [n_heads, head_dim]
pub fn flash_attention_multi_head(
    q_state: &Array2<f32>,
    k_cache: ArrayView2<f32>, // Slice for one KV head: [seq_len, head_dim]
    v_cache: ArrayView2<f32>, // Slice for one KV head: [seq_len, head_dim]
    n_heads: usize,
    n_kv_heads: usize,
    seq_len: usize,
    config: &FlashAttentionConfig,
) -> Array2<f32> {
    let (_, head_dim) = q_state.dim();
    let scale = 1.0 / (head_dim as f32).sqrt();
    let kv_groups = n_heads / n_kv_heads;

    let mut output = Array2::zeros((n_heads, head_dim));

    // Process each query head
    for h in 0..n_heads {
        let _kv_head = h / kv_groups;
        let q_row = q_state.row(h);
        let q_slice = q_row.as_slice().expect("q_state row must be contiguous");

        // Compute flash attention for this head
        let head_output =
            flash_attention_single_head(q_slice, k_cache, v_cache, seq_len, scale, config);

        // Copy to output
        for d in 0..head_dim {
            output[[h, d]] = head_output[d];
        }
    }

    output
}

/// Flash Attention with parallel head computation.
///
/// Same as `flash_attention_multi_head` but processes heads in parallel
/// using Rayon.
#[cfg(feature = "parallel")]
pub fn flash_attention_parallel(
    q_state: &Array2<f32>,
    k_cache: ArrayView2<f32>,
    v_cache: ArrayView2<f32>,
    n_heads: usize,
    n_kv_heads: usize,
    seq_len: usize,
    config: &FlashAttentionConfig,
) -> Array2<f32> {
    use rayon::prelude::*;

    let (_, head_dim) = q_state.dim();
    let scale = 1.0 / (head_dim as f32).sqrt();
    let _kv_groups = n_heads / n_kv_heads;

    // Compute each head in parallel
    let head_outputs: Vec<(usize, Vec<f32>)> = (0..n_heads)
        .into_par_iter()
        .map(|h| {
            let q_row = q_state.row(h);
            let q_slice = q_row.as_slice().expect("q_state row must be contiguous");

            let output =
                flash_attention_single_head(q_slice, k_cache, v_cache, seq_len, scale, config);

            (h, output)
        })
        .collect();

    // Assemble output
    let mut output = Array2::zeros((n_heads, head_dim));
    for (h, head_output) in head_outputs {
        for d in 0..head_dim {
            output[[h, d]] = head_output[d];
        }
    }

    output
}

/// Optimized flash attention that writes directly into pre-allocated output buffer.
///
/// This variant avoids allocation by writing results directly into the output buffer.
///
/// # Arguments
/// * `query` - Query vector [head_dim]
/// * `k_cache` - Key cache [seq_len, head_dim]
/// * `v_cache` - Value cache [seq_len, head_dim]
/// * `output` - Pre-allocated output buffer [head_dim]
/// * `seq_len` - Current sequence length
/// * `scale` - Attention scale factor
/// * `config` - Flash attention configuration
/// * `scores_buffer` - Pre-allocated scores buffer [tile_size]
/// * `values_buffer` - Pre-allocated values buffer [tile_size * head_dim]
#[allow(clippy::too_many_arguments)]
pub fn flash_attention_into(
    query: &[f32],
    k_cache: ArrayView2<f32>,
    v_cache: ArrayView2<f32>,
    output: &mut [f32],
    seq_len: usize,
    scale: f32,
    config: &FlashAttentionConfig,
    scores_buffer: &mut [f32],
    values_buffer: &mut [f32],
) {
    let head_dim = query.len();
    let tile_size = config.tile_size.min(seq_len);

    // Initialize output to zero
    for o in output.iter_mut() {
        *o = 0.0;
    }

    // Use standard attention for short sequences
    if seq_len < config.threshold {
        standard_attention_into(query, k_cache, v_cache, output, seq_len, scale);
        return;
    }

    // Online softmax state
    let mut max_val = f32::NEG_INFINITY;
    let mut sum = 0.0f32;

    // Process K/V cache in tiles
    let mut pos = 0;
    while pos < seq_len {
        let tile_end = (pos + tile_size).min(seq_len);
        let current_tile_size = tile_end - pos;

        // Compute attention scores for this tile
        for i in 0..current_tile_size {
            let k_row = k_cache.row(pos + i);
            let k_slice = k_row.as_slice().expect("k_cache row must be contiguous");
            let mut dot = 0.0f32;
            for d in 0..head_dim {
                dot += query[d] * k_slice[d];
            }
            scores_buffer[i] = dot * scale;
        }

        // Copy values for this tile
        for i in 0..current_tile_size {
            let v_row = v_cache.row(pos + i);
            let v_slice = v_row.as_slice().expect("v_cache row must be contiguous");
            for d in 0..head_dim {
                values_buffer[i * head_dim + d] = v_slice[d];
            }
        }

        // Find max in this tile
        let tile_max = scores_buffer
            .iter()
            .take(current_tile_size)
            .copied()
            .fold(f32::NEG_INFINITY, f32::max);

        // Update global max and rescale if needed
        let new_max = max_val.max(tile_max);
        if new_max > max_val && sum > 0.0 {
            let scale_factor = (max_val - new_max).exp();
            sum *= scale_factor;
            for o in output.iter_mut() {
                *o *= scale_factor;
            }
        }

        // Accumulate weighted values
        let mut tile_sum = 0.0f32;
        for i in 0..current_tile_size {
            let weight = (scores_buffer[i] - new_max).exp();
            tile_sum += weight;
            for d in 0..head_dim {
                output[d] += weight * values_buffer[i * head_dim + d];
            }
        }

        sum += tile_sum;
        max_val = new_max;
        pos = tile_end;
    }

    // Normalize
    if sum > 0.0 {
        let inv_sum = 1.0 / sum;
        for o in output.iter_mut() {
            *o *= inv_sum;
        }
    }
}

/// Standard attention that writes directly into output buffer.
fn standard_attention_into(
    query: &[f32],
    k_cache: ArrayView2<f32>,
    v_cache: ArrayView2<f32>,
    output: &mut [f32],
    seq_len: usize,
    scale: f32,
) {
    let head_dim = query.len();

    // Allocate scores (unavoidable for standard attention)
    let mut scores = vec![0.0f32; seq_len];

    // Compute scores
    for i in 0..seq_len {
        let k_row = k_cache.row(i);
        let k_slice = k_row.as_slice().expect("k_cache row must be contiguous");
        let mut dot = 0.0f32;
        for d in 0..head_dim {
            dot += query[d] * k_slice[d];
        }
        scores[i] = dot * scale;
    }

    // Softmax
    let max_score = scores.iter().copied().fold(f32::NEG_INFINITY, f32::max);
    let mut sum = 0.0f32;
    for s in scores.iter_mut() {
        *s = (*s - max_score).exp();
        sum += *s;
    }
    let inv_sum = 1.0 / sum;
    for s in scores.iter_mut() {
        *s *= inv_sum;
    }

    // Weighted sum into output
    output.fill(0.0);
    for i in 0..seq_len {
        let v_row = v_cache.row(i);
        let v_slice = v_row.as_slice().expect("v_cache row must be contiguous");
        let w = scores[i];
        for d in 0..head_dim {
            output[d] += w * v_slice[d];
        }
    }
}

/// Estimate memory usage of flash attention vs standard attention.
///
/// # Arguments
/// * `seq_len` - Sequence length
/// * `n_heads` - Number of attention heads
/// * `head_dim` - Dimension per head
/// * `tile_size` - Flash attention tile size
///
/// # Returns
/// (flash_bytes, standard_bytes) - Memory estimates in bytes
///
/// # Note
/// Flash attention processes heads sequentially, reusing buffers between heads.
/// Standard attention typically materializes all attention scores for all heads.
pub fn estimate_memory(
    seq_len: usize,
    n_heads: usize,
    head_dim: usize,
    tile_size: usize,
) -> (usize, usize) {
    let float_size = std::mem::size_of::<f32>();

    // Standard attention: n_heads * seq_len scores + output context
    let standard_scores = n_heads * seq_len * float_size;
    let standard_context = n_heads * head_dim * float_size;
    let standard_total = standard_scores + standard_context;

    // Flash attention: single-head tile buffers (reused across heads)
    // Only needs buffers for one head at a time when processing sequentially
    let flash_scores = tile_size * float_size;
    let flash_values = tile_size * head_dim * float_size;
    let flash_output = head_dim * float_size;
    let flash_per_head = flash_scores + flash_values + flash_output;
    // Plus output context for all heads
    let flash_total = flash_per_head + (n_heads * head_dim * float_size);

    (flash_total, standard_total)
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::Array2;

    fn make_test_data(seq_len: usize, head_dim: usize) -> (Vec<f32>, Array2<f32>, Array2<f32>) {
        let query: Vec<f32> = (0..head_dim).map(|i| (i as f32 * 0.1).sin()).collect();

        let mut k_cache = Array2::zeros((seq_len, head_dim));
        let mut v_cache = Array2::zeros((seq_len, head_dim));

        for i in 0..seq_len {
            for d in 0..head_dim {
                k_cache[[i, d]] = ((i * head_dim + d) as f32 * 0.05).cos();
                v_cache[[i, d]] = ((i * head_dim + d) as f32 * 0.03).sin();
            }
        }

        (query, k_cache, v_cache)
    }

    #[test]
    fn test_flash_attention_matches_standard() {
        let seq_len = 200;
        let head_dim = 64;
        let (query, k_cache, v_cache) = make_test_data(seq_len, head_dim);
        let scale = 1.0 / (head_dim as f32).sqrt();

        let config = FlashAttentionConfig::default();

        // Flash attention
        let flash_output = flash_attention_single_head(
            &query,
            k_cache.view(),
            v_cache.view(),
            seq_len,
            scale,
            &config,
        );

        // Standard attention
        let standard_output =
            standard_attention_single_head(&query, k_cache.view(), v_cache.view(), seq_len, scale);

        // Compare outputs
        for d in 0..head_dim {
            let diff = (flash_output[d] - standard_output[d]).abs();
            assert!(
                diff < 1e-4,
                "Mismatch at dim {}: flash={}, standard={}, diff={}",
                d,
                flash_output[d],
                standard_output[d],
                diff
            );
        }
    }

    #[test]
    fn test_flash_attention_into() {
        let seq_len = 150;
        let head_dim = 64;
        let (query, k_cache, v_cache) = make_test_data(seq_len, head_dim);
        let scale = 1.0 / (head_dim as f32).sqrt();

        let config = FlashAttentionConfig::default();
        let tile_size = config.tile_size;

        let mut output = vec![0.0f32; head_dim];
        let mut scores_buffer = vec![0.0f32; tile_size];
        let mut values_buffer = vec![0.0f32; tile_size * head_dim];

        flash_attention_into(
            &query,
            k_cache.view(),
            v_cache.view(),
            &mut output,
            seq_len,
            scale,
            &config,
            &mut scores_buffer,
            &mut values_buffer,
        );

        // Compare with standard
        let standard =
            standard_attention_single_head(&query, k_cache.view(), v_cache.view(), seq_len, scale);

        for d in 0..head_dim {
            let diff = (output[d] - standard[d]).abs();
            assert!(
                diff < 1e-4,
                "Mismatch at dim {}: got={}, expected={}",
                d,
                output[d],
                standard[d]
            );
        }
    }

    #[test]
    fn test_online_softmax_state() {
        let head_dim = 4;
        let mut state = OnlineSoftmaxState::new(head_dim);

        // First tile: scores [1, 2], values [[1,1,1,1], [2,2,2,2]]
        let scores1 = vec![1.0, 2.0];
        let values1 = vec![1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0];
        state.update_tile(&scores1, &values1, head_dim, 2);

        // Second tile: scores [3], values [[3,3,3,3]]
        let scores2 = vec![3.0];
        let values2 = vec![3.0, 3.0, 3.0, 3.0];
        state.update_tile(&scores2, &values2, head_dim, 1);

        state.finalize();

        // Verify output sums to a weighted average
        let output = state.output();

        // Manual computation
        let max_val = 3.0f32;
        let e1 = (1.0 - max_val).exp();
        let e2 = (2.0 - max_val).exp();
        let e3 = (3.0 - max_val).exp();
        let sum = e1 + e2 + e3;
        let expected = (e1 * 1.0 + e2 * 2.0 + e3 * 3.0) / sum;

        for &o in output {
            assert!(
                (o - expected).abs() < 1e-5,
                "expected {}, got {}",
                expected,
                o
            );
        }
    }

    #[test]
    fn test_memory_estimate() {
        let seq_len = 1024;
        let n_heads = 32;
        let head_dim = 128;
        let tile_size = 64;

        let (flash_bytes, standard_bytes) = estimate_memory(seq_len, n_heads, head_dim, tile_size);

        // Flash should use less memory for long sequences
        assert!(
            flash_bytes < standard_bytes,
            "Flash ({} bytes) should be less than standard ({} bytes)",
            flash_bytes,
            standard_bytes
        );
    }

    #[test]
    fn test_short_sequence_fallback() {
        let seq_len = 50; // Below threshold
        let head_dim = 64;
        let (query, k_cache, v_cache) = make_test_data(seq_len, head_dim);
        let scale = 1.0 / (head_dim as f32).sqrt();

        let config = FlashAttentionConfig::default();

        // Should use standard attention
        let output = flash_attention_single_head(
            &query,
            k_cache.view(),
            v_cache.view(),
            seq_len,
            scale,
            &config,
        );

        // Should still produce correct results
        assert_eq!(output.len(), head_dim);

        // Verify output is normalized (softmax property)
        let sum: f32 = output.iter().map(|x| x.abs()).sum();
        assert!(sum > 0.0, "Output should not be all zeros");
    }
}
